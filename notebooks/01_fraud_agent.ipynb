{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b79b44-e6a0-461f-955f-03bc881e78db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚ú®‚ú®‚ú® VERSION 2.1 - CREATED DEC 14, 2024 ‚ú®‚ú®‚ú®\n",
    "# LangGraph Agent Learning - Healthcare Fraud Detection\n",
    "\n",
    "**Purpose:** Learn LangGraph ReAct agents by wrapping existing UC Functions, Vector Search, and Genie API\n",
    "\n",
    "**Domain:** Healthcare insurance claims (Upcoding, Phantom Billing, Provider Networks)\n",
    "\n",
    "**Authentication:** Uses WorkspaceClient (same as dashboard) for realistic testing\n",
    "\n",
    "**Branch:** `main` (production-ready demo)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ **NOTEBOOK VERSION INFO**\n",
    "\n",
    "**üìÖ Last Updated:** December 14, 2024\n",
    "**üîß Version:** 2.1 - WITH ARCHITECTURE DIAGRAMS\n",
    "**‚ú® New in 2.1:** Visual architecture diagrams throughout notebook\n",
    "**‚úÖ Status:** Production-ready educational demo\n",
    "\n",
    "**üö® VERIFICATION:** After running LLM init cell, you MUST see:\n",
    "```\n",
    "‚úÖ Tools bound to LLM (ensures proper JSON format)\n",
    "```\n",
    "\n",
    "**If you DON'T see this line ‚Üí Detach & Reattach notebook!**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. ‚úÖ Tests all 4 tools individually (UC Functions, Vector Search, Genie)\n",
    "2. ‚úÖ Wraps them as LangChain Tools\n",
    "3. ‚úÖ Creates LangGraph ReAct Agent\n",
    "4. ‚úÖ Tests agent with different claim complexities (simple ‚Üí suspicious ‚Üí complex)\n",
    "5. ‚úÖ Compares Sequential vs Agent approaches (cost optimization)\n",
    "\n",
    "**Goal:** Demonstrate how the fraud detection agent works under the hood\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Tutorial Architecture Overview\n",
    "\n",
    "![5-Layer Architecture](/Workspace/Users/vik.malhotra@databricks.com/fraud_detection/images/technical_architecture_5layers.png)\n",
    "\n",
    "*This tutorial teaches you to build the complete 5-layer architecture: from Databricks services (bottom) through LangChain tools, agents, ReAct pattern, up to LangGraph orchestration (top)*\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è The Four Tools You'll Build\n",
    "\n",
    "![Four Tool Architecture](/Workspace/Users/vik.malhotra@databricks.com/fraud_detection/images/4_tools_architecture.png)\n",
    "\n",
    "*Each tool wraps a Databricks service, giving the agent different capabilities: classification, extraction, search, and analytics*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481c0ee8-4b30-440f-b6f6-89cb02fdd677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üì¶ Setup - Install Dependencies\n",
    "\n",
    "**Why these specific versions?**\n",
    "- `langgraph>=1.0.0` - Latest version (v1.0) removed `state_modifier` parameter\n",
    "- `langchain>=0.3.0` - Latest version with improved tool calling\n",
    "- Pinning versions ensures reproducibility across environments\n",
    "\n",
    "**Key packages:**\n",
    "- `langgraph` - State machine and agent orchestration\n",
    "- `langchain` - Agent framework and tool abstractions\n",
    "- `databricks-langchain` - Databricks-specific LangChain integrations\n",
    "- `unitycatalog-langchain` - Unity Catalog function integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109c7f0c-c514-4c72-999e-e7cfbff2066e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "databricks-sirens 0.6.8 requires pyarrow==20.0.0, but you have pyarrow 14.0.2 which is incompatible.\n",
      "databricks-sirens 0.6.8 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "dbt-adapters 1.9.0 requires protobuf<5.0,>=3.0, but you have protobuf 5.29.5 which is incompatible.\n",
      "dbt-common 1.12.0 requires protobuf<5.0.0,>=4.0.0, but you have protobuf 5.29.5 which is incompatible.\n",
      "dbt-core 1.8.7 requires protobuf<5,>=4.0.0, but you have protobuf 5.29.5 which is incompatible.\n",
      "dbt-databricks 1.8.7 requires databricks-sdk==0.17.0, but you have databricks-sdk 0.73.0 which is incompatible.\n",
      "dbt-databricks 1.8.7 requires protobuf<5.0.0, but you have protobuf 5.29.5 which is incompatible.\n",
      "timezonefinder 6.5.3 requires cffi<2,>=1.15.1, but you have cffi 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph>=1.0.0 langchain>=0.3.0 langchain-core>=0.3.0 langchain-community>=0.3.0 databricks-langchain unitycatalog-langchain[databricks] backoff databricks-sdk mlflow databricks-vectorsearch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbdd218f-9b4c-4949-95af-660f5f0b0c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a6e155c-b42a-4282-aaa7-a14fd47bc5e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Version Check - Run This First!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380688da-c573-4097-8576-4863e154a767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç NOTEBOOK VERSION CHECK\n",
      "================================================================================\n",
      "üìÖ Deployed Version: December 7, 2024 - 7:30 PM PST\n",
      "üîß Version: 2.1 - WITH ARCHITECTURE DIAGRAMS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ If you see this, the notebook file is the latest version!\n",
      "üé® NEW: Architecture diagrams embedded throughout notebook\n",
      "üö® After running LLM init, you MUST see: 'Tools bound to LLM'\n",
      "‚ùå If you don't see that line ‚Üí Detach & Reattach notebook\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Version check - Run this to verify you have the latest notebook\n",
    "import datetime\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç NOTEBOOK VERSION CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üìÖ Deployed Version: December 7, 2024 - 7:30 PM PST\")\n",
    "print(\"üîß Version: 2.1 - WITH ARCHITECTURE DIAGRAMS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ If you see this, the notebook file is the latest version!\")\n",
    "print(\"üé® NEW: Architecture diagrams embedded throughout notebook\")\n",
    "print(\"üö® After running LLM init, you MUST see: 'Tools bound to LLM'\")\n",
    "print(\"‚ùå If you don't see that line ‚Üí Detach & Reattach notebook\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3409f8-aa2e-4f1f-b800-8d2958247c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîß Imports and Configuration\n",
    "\n",
    "**Import Strategy:**\n",
    "- `WorkspaceClient` - Databricks SDK for calling APIs (UC Functions, Genie, etc.)\n",
    "- `ChatDatabricks` - LangChain wrapper for Databricks Foundation Models\n",
    "- `VectorSearchClient` - For semantic search in fraud knowledge base\n",
    "- `SystemMessage` - LangChain message type for system prompts (v1.0+ pattern)\n",
    "- `create_react_agent` - Pre-built ReAct agent from LangGraph\n",
    "\n",
    "**Why these imports?**\n",
    "- All use WorkspaceClient pattern (same as dashboard) for portability\n",
    "- SystemMessage needed for manual prompt injection (v1.0+ requirement)\n",
    "- No custom state classes needed for this simple use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20abcc4-7597-402f-8ab3-2bf2eca75f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WorkspaceClient initialized\n",
      "üìç Workspace: https://eastus2.azuredatabricks.net\n",
      "üë§ Current user: animesh.jha@databricks.com\n"
     ]
    }
   ],
   "source": [
    "# Core Databricks integrations\n",
    "from databricks.sdk import WorkspaceClient  # Unified API client\n",
    "from databricks_langchain import ChatDatabricks  # LLM wrapper\n",
    "from databricks.vector_search.client import VectorSearchClient  # Semantic search\n",
    "\n",
    "# LangChain/LangGraph components\n",
    "from langchain_core.tools import Tool  # Tool wrapper abstraction\n",
    "from langchain_core.messages import SystemMessage  # For system prompt injection\n",
    "from langgraph.prebuilt import create_react_agent  # Pre-built ReAct pattern\n",
    "\n",
    "# Additional imports for state management (if needed for advanced use)\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import backoff  # For retry logic\n",
    "\n",
    "# Initialize WorkspaceClient (same pattern as dashboard)\n",
    "w = WorkspaceClient()\n",
    "\n",
    "print(\"‚úÖ WorkspaceClient initialized\")\n",
    "print(f\"üìç Workspace: {w.config.host}\")\n",
    "print(f\"üë§ Current user: {w.current_user.me().user_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a35ad9-6f6f-452a-8097-64d4f4cf5ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öôÔ∏è Configuration Variables\n",
    "\n",
    "These match the dashboard configuration\n",
    "\n",
    "**Configuration Strategy:**\n",
    "- Centralized config makes it easy to change environments (dev/prod)\n",
    "- All IDs/names are explicit (no magic strings scattered in code)\n",
    "- LLM endpoint changed to Meta Llama 3.1 8B (available in workspace)\n",
    "\n",
    "**Why Meta Llama 3.1 8B Instruct?**\n",
    "- Foundation model (managed by Databricks, always available)\n",
    "- Good balance: fast (8B params) + capable (instruction-tuned)\n",
    "- Strong tool-calling abilities needed for ReAct claims_analysis\n",
    "- Cost-effective for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7882107e-b81c-4587-9963-32d781368323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úÖ CONFIGURATION LOADED\n",
      "================================================================================\n",
      "üìö Catalog: langraph_demo\n",
      "üìä Schema: agents\n",
      "üîç Vector Index: langraph_demo.agents.knowledge_base_index\n",
      "ü§ñ Genie Space: 01f0d3b893281964829ca73bcfa4d7c7\n",
      "üß† LLM Endpoint: databricks-claude-sonnet-4\n",
      "================================================================================\n",
      "‚úÖ VERIFICATION PASSED: Using Claude Sonnet 4 (correct model for function calling)\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Tutorial environment (independent from main project)\n",
    "# Get catalog from widgets if available (set by DAB), otherwise use default\n",
    "try:\n",
    "    CATALOG = dbutils.widgets.get(\"catalog\")\n",
    "except:\n",
    "    CATALOG = \"fraud_detection_dev\"  # Test/demo catalog (safe default)\n",
    "\n",
    "SCHEMA = \"claims_analysis\"  # Schema containing tables and functions\n",
    "WAREHOUSE_ID = \"148ccb90800933a1\"  # ‚Üê UPDATE THIS: Your SQL Warehouse ID\n",
    "INDEX_NAME = f\"{CATALOG}.{SCHEMA}.knowledge_base_index\"  # Vector search index\n",
    "GENIE_SPACE_ID = \"01f0d3b893281964829ca73bcfa4d7c7\"  # ‚Üê UPDATE THIS: Copy from Genie UI after creating space\n",
    "\n",
    "# LLM Configuration\n",
    "# üö® CRITICAL: Use Claude Sonnet 4 for production claims_analysis with function calling\n",
    "# \n",
    "# LESSON LEARNED:\n",
    "# - Meta Llama models return XML-like syntax: <function=name>{args}</function>\n",
    "# - LangGraph expects proper JSON format for tool calls\n",
    "# - This causes BAD_REQUEST errors: \"Model response did not respect required format\"\n",
    "# \n",
    "# MODEL COMPARISON (Function Calling):\n",
    "# ‚úÖ Claude Sonnet 4: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê BEST - Perfect JSON, reliable tool use\n",
    "# ‚úÖ GPT-4: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Also excellent (not tested here)\n",
    "# ‚ö†Ô∏è DBRX: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ Good (but unavailable in this workspace)\n",
    "# ‚ùå Meta Llama 3.3 70B: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ Poor - XML format issues\n",
    "# ‚ùå Meta Llama 3.1 8B: ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ Very Poor - XML format issues\n",
    "#\n",
    "# COST vs RELIABILITY:\n",
    "# - Claude costs more per token BUT first-try success = lower total cost\n",
    "# - Failed tool calls waste tokens on retries + bad UX\n",
    "# - For production claims_analysis: Reliability > per-token cost\n",
    "LLM_ENDPOINT = \"databricks-claude-sonnet-4\"  # ‚úÖ PRODUCTION CHOICE\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ CONFIGURATION LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìö Catalog: {CATALOG}\")\n",
    "print(f\"üìä Schema: {SCHEMA}\")\n",
    "print(f\"üîç Vector Index: {INDEX_NAME}\")\n",
    "print(f\"ü§ñ Genie Space: {GENIE_SPACE_ID}\")\n",
    "print(f\"üß† LLM Endpoint: {LLM_ENDPOINT}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# üö® CRITICAL VERIFICATION: Ensure we're using Claude Sonnet 4\n",
    "if LLM_ENDPOINT != \"databricks-claude-sonnet-4\":\n",
    "    print(\"\\n\" + \"!\" * 80)\n",
    "    print(\"‚ö†Ô∏è  WARNING: WRONG MODEL DETECTED!\")\n",
    "    print(\"!\" * 80)\n",
    "    print(f\"‚ùå Currently using: {LLM_ENDPOINT}\")\n",
    "    print(f\"‚úÖ Should be using: databricks-claude-sonnet-4\")\n",
    "    print(\"\\nüîß FIX:\")\n",
    "    print(\"   1. In Databricks UI: Detach & Reattach this notebook\")\n",
    "    print(\"   2. OR: Click 'Clear' ‚Üí 'Clear State & Outputs'\")\n",
    "    print(\"   3. Re-run ALL cells from the beginning\")\n",
    "    print(\"!\" * 80)\n",
    "    raise ValueError(\n",
    "        f\"Wrong LLM endpoint! Using '{LLM_ENDPOINT}' instead of 'databricks-claude-sonnet-4'. \"\n",
    "        f\"This will cause BAD_REQUEST errors with XML-format tool calls. \"\n",
    "        f\"Detach & reattach notebook, then re-run from beginning.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úÖ VERIFICATION PASSED: Using Claude Sonnet 4 (correct model for function calling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5256cc30-cb16-432e-a1b0-4edaa46fb612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 1: Test Individual Tools\n",
    "\n",
    "Before creating the agent, verify each tool works correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7b092f-013f-46ce-863a-62e2570a4e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîß Tool 1: UC Function Wrapper (Classification & Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c918ee5-3eac-4694-bd3d-d0b9297e9823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def call_uc_function(function_name: str, parameters: dict):\n",
    "    \"\"\"\n",
    "    Call a UC function using Statement Execution API (same as dashboard).\n",
    "    This is more reliable for Databricks Apps and notebooks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from databricks.sdk.service.sql import StatementState\n",
    "        \n",
    "        print(f\"üîß Calling UC Function: {CATALOG}.{SCHEMA}.{function_name}\")\n",
    "        \n",
    "        # Build SQL query with parameters\n",
    "        param_values = []\n",
    "        for key, value in parameters.items():\n",
    "            if isinstance(value, str):\n",
    "                # Escape single quotes\n",
    "                escaped = value.replace(\"'\", \"''\")\n",
    "                param_values.append(f\"'{escaped}'\")\n",
    "            else:\n",
    "                param_values.append(str(value))\n",
    "        \n",
    "        args_str = ', '.join(param_values)\n",
    "        query = f\"SELECT {CATALOG}.{SCHEMA}.{function_name}({args_str}) as result\"\n",
    "        \n",
    "        print(f\"  üìù SQL: {query[:100]}...\")\n",
    "        \n",
    "        # Execute via Statement Execution API\n",
    "        response = w.statement_execution.execute_statement(\n",
    "            warehouse_id=WAREHOUSE_ID,\n",
    "            statement=query,\n",
    "            wait_timeout='30s'\n",
    "        )\n",
    "        \n",
    "        if response.status.state == StatementState.SUCCEEDED:\n",
    "            # Extract result from response\n",
    "            if response.result and response.result.data_array:\n",
    "                result_json = response.result.data_array[0][0]\n",
    "                result = json.loads(result_json) if isinstance(result_json, str) else result_json\n",
    "                print(f\"‚úÖ UC Function result received\")\n",
    "                return result\n",
    "            else:\n",
    "                print(f\"‚ùå No data in response\")\n",
    "                return {\"error\": \"No data returned\"}\n",
    "        else:\n",
    "            error_msg = response.status.error.message if response.status.error else \"Unknown error\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return {\"error\": error_msg}\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Exception calling UC function: {str(e)}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": error_msg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb343ff-c439-4154-94af-377d84a13483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: UC Function - ai_classify\n",
      "================================================================================\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_classify\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_classify('Database connection timeout in production affecting all use...\n",
      "‚úÖ UC Function result received\n",
      "\n",
      "üìä Classification Result:\n",
      "{\n",
      "  \"category\": \"Software\",\n",
      "  \"priority\": \"P3\",\n",
      "  \"assigned_team\": \"Applications\",\n",
      "  \"confidence\": \"0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 1: fraud_classify\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: UC Function - fraud_classify\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_claim = \"Database connection timeout in production affecting all users\"\n",
    "classification_result = call_uc_function(\n",
    "    \"fraud_classify\",\n",
    "    {\"claim_text\": test_claim}\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Classification Result:\")\n",
    "print(json.dumps(classification_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ccc89b-00cd-4181-907f-668dd2f0dbf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 2: UC Function - ai_extract\n",
      "================================================================================\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_extract\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_extract('Database connection timeout in production affecting all user...\n",
      "‚úÖ UC Function result received\n",
      "\n",
      "üìä Metadata Extraction Result:\n",
      "{\n",
      "  \"requires_escalation\": \"false\",\n",
      "  \"priority_score\": \"0.5\",\n",
      "  \"technical_keywords\": [],\n",
      "  \"urgency_level\": \"Medium\",\n",
      "  \"affected_systems\": [],\n",
      "  \"user_impact\": \"Single User\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 2: fraud_extract_indicators\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 2: UC Function - fraud_extract_indicators\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metadata_result = call_uc_function(\n",
    "    \"fraud_extract_indicators\",\n",
    "    {\"claim_text\": test_claim}\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Metadata Extraction Result:\")\n",
    "print(json.dumps(metadata_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4b2a0a-4d53-4cc1-aaf8-e10267135b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Tool 2: Vector Search Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eca225d-5f24-47c5-8eb7-7374a6abbc02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def search_knowledge_base(query: str, num_results: int = 3):\n",
    "    \"\"\"\n",
    "    Search fraud knowledge base using Vector Search.\n",
    "    Same pattern as dashboard - directly portable!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîç Searching fraud knowledge base: '{query[:50]}...'\")\n",
    "        \n",
    "        # Initialize VectorSearchClient (uses workspace client from environment)\n",
    "        vsc = VectorSearchClient()\n",
    "        \n",
    "        results = vsc.get_index(\n",
    "            index_name=INDEX_NAME\n",
    "        ).similarity_search(\n",
    "            query_text=query,\n",
    "            columns=[\"doc_id\", \"doc_type\", \"title\", \"content\"],\n",
    "            num_results=num_results\n",
    "        )\n",
    "        \n",
    "        docs = results.get('result', {}).get('data_array', [])\n",
    "        print(f\"‚úÖ Found {len(docs)} documents\")\n",
    "        \n",
    "        formatted_results = [\n",
    "            {\n",
    "                \"doc_id\": doc[0],\n",
    "                \"doc_type\": doc[1],\n",
    "                \"title\": doc[2],\n",
    "                \"content\": doc[3][:200] + \"...\" if len(doc[3]) > 200 else doc[3],\n",
    "                \"score\": doc[4] if len(doc) > 4 else None\n",
    "            }\n",
    "            for doc in docs\n",
    "        ]\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Vector Search Error: {str(e)}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6541ad-307b-4121-9ef2-5bba840d22b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 3: Vector Search\n",
      "================================================================================\n",
      "üîç Searching knowledge base: 'database timeout connection pool configuration...'\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "‚úÖ Found 3 documents\n",
      "\n",
      "üìö Knowledge Base Results (3 documents):\n",
      "\n",
      "1. üìÑ DATABASE ADMINISTRATION GUIDE - DATABASE CONNECTION ISSUES (Part 2)\n",
      "   üìÇ Type: Guide\n",
      "   üìù Content: ---------------------------\n",
      "\n",
      "Connection Timeout (Priority P1):\n",
      "- Check database server status\n",
      "- Verify connection pool settings\n",
      "- Review network conne...\n",
      "   üéØ Score: 0.7098\n",
      "\n",
      "2. üìÑ IT INFRASTRUCTURE RUNBOOK - SECTION 3: DATABASE TROUBLESHOOTING (Part 3)\n",
      "   üìÇ Type: Runbook\n",
      "   üìù Content: -------------------------------------------\n",
      "\n",
      "3.1 Slow Query Performance\n",
      "Priority: P2 (High if affecting user experience)\n",
      "Affected Systems: Database se...\n",
      "   üéØ Score: 0.6183\n",
      "\n",
      "3. üìÑ IT INFRASTRUCTURE RUNBOOK - SECTION 5: COMMON ERROR CODES (Part 5)\n",
      "   üìÇ Type: Runbook\n",
      "   üìù Content: -------------------------------------------\n",
      "\n",
      "HTTP 500: Internal Server Error\n",
      "- Check application logs immediately\n",
      "- Review recent deployments\n",
      "- Verify...\n",
      "   üéØ Score: 0.5815\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Vector Search\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 3: Vector Search\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "search_query = \"database timeout connection pool configuration\"\n",
    "search_results = search_knowledge_base(search_query)\n",
    "\n",
    "print(f\"\\nüìö Knowledge Base Results ({len(search_results)} documents):\")\n",
    "for i, doc in enumerate(search_results, 1):\n",
    "    print(f\"\\n{i}. üìÑ {doc['title']}\")\n",
    "    print(f\"   üìÇ Type: {doc['doc_type']}\")\n",
    "    print(f\"   üìù Content: {doc['content'][:150]}...\")\n",
    "    if doc.get('score'):\n",
    "        print(f\"   üéØ Score: {doc['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317fe146-fcc7-4a51-a773-79030411f811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü§ñ Tool 3: Genie Conversation API Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c9c383c-d067-42a6-a943-c59a1815fb2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GenieConversationTool:\n",
    "    \"\"\"\n",
    "    Genie Conversation API wrapper using WorkspaceClient.\n",
    "    Same class as dashboard - directly portable!\n",
    "    \"\"\"\n",
    "    def __init__(self, workspace_client: WorkspaceClient, space_id: str):\n",
    "        self.w = workspace_client\n",
    "        self.space_id = space_id\n",
    "        print(f\"[Genie] Initialized with space_id: {space_id}\")\n",
    "    \n",
    "    def start_conversation(self, question: str):\n",
    "        \"\"\"Start a new Genie conversation\"\"\"\n",
    "        try:\n",
    "            print(f\"[Genie] Starting conversation...\")\n",
    "            response = self.w.api_client.do(\n",
    "                'POST',\n",
    "                f'/api/2.0/genie/spaces/{self.space_id}/start-conversation',\n",
    "                body={'content': question}\n",
    "            )\n",
    "            \n",
    "            conversation_id = response.get('conversation_id')\n",
    "            message_id = response.get('message_id')\n",
    "            \n",
    "            print(f\"[Genie] ‚úÖ Started - conversation_id: {conversation_id}, message_id: {message_id}\")\n",
    "            \n",
    "            return {\n",
    "                'status': 'started',\n",
    "                'conversation_id': conversation_id,\n",
    "                'message_id': message_id\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[Genie] ‚ùå Error starting conversation: {str(e)}\")\n",
    "            return {'status': 'error', 'error': str(e)}\n",
    "    \n",
    "    def poll_for_result(self, conversation_id: str, message_id: str, max_wait_seconds: int = 120):\n",
    "        \"\"\"Poll for query completion with exponential backoff\"\"\"\n",
    "        print(f\"[Genie] Polling for result (max {max_wait_seconds}s)...\")\n",
    "        start_time = time.time()\n",
    "        poll_interval = 2\n",
    "        poll_count = 0\n",
    "        \n",
    "        while time.time() - start_time < max_wait_seconds:\n",
    "            try:\n",
    "                poll_count += 1\n",
    "                response = self.w.api_client.do(\n",
    "                    'GET',\n",
    "                    f'/api/2.0/genie/spaces/{self.space_id}/conversations/{conversation_id}/messages/{message_id}'\n",
    "                )\n",
    "                \n",
    "                status = response.get('status')\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"[Genie] Poll #{poll_count} ({elapsed:.1f}s): Status = {status}\")\n",
    "                \n",
    "                if status == 'COMPLETED':\n",
    "                    print(f\"[Genie] ‚úÖ Query completed!\")\n",
    "                    return {'status': 'completed', 'response': response}\n",
    "                elif status == 'FAILED':\n",
    "                    error = response.get('error', {})\n",
    "                    print(f\"[Genie] ‚ùå Query failed: {error}\")\n",
    "                    return {'status': 'failed', 'response': response, 'error': error}\n",
    "                \n",
    "                time.sleep(poll_interval)\n",
    "                poll_interval = min(poll_interval * 1.2, 10)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[Genie] ‚ö†Ô∏è Poll error: {str(e)}\")\n",
    "                time.sleep(poll_interval)\n",
    "        \n",
    "        print(f\"[Genie] ‚è±Ô∏è Timeout after {max_wait_seconds}s\")\n",
    "        return {'status': 'timeout', 'error': f'Query did not complete within {max_wait_seconds}s'}\n",
    "    \n",
    "    def query(self, question: str):\n",
    "        \"\"\"\n",
    "        Complete Genie query workflow: start ‚Üí poll ‚Üí extract results ‚Üí fetch data\n",
    "        Returns structured results with actual data rows or error message.\n",
    "        \"\"\"\n",
    "        print(f\"\\n[Genie] === Starting Genie Query ===\")\n",
    "        print(f\"[Genie] Question: {question[:100]}...\")\n",
    "        \n",
    "        # Step 1: Start conversation\n",
    "        start_result = self.start_conversation(question)\n",
    "        if start_result.get('status') != 'started':\n",
    "            error_msg = f\"Error starting conversation: {start_result.get('error')}\"\n",
    "            print(f\"[Genie] {error_msg}\")\n",
    "            return {\"error\": error_msg}\n",
    "        \n",
    "        conversation_id = start_result['conversation_id']\n",
    "        message_id = start_result['message_id']\n",
    "        \n",
    "        # Step 2: Poll for completion\n",
    "        poll_result = self.poll_for_result(conversation_id, message_id, max_wait_seconds=120)\n",
    "        \n",
    "        poll_status = poll_result.get('status')\n",
    "        \n",
    "        if poll_status == 'failed':\n",
    "            error_detail = poll_result.get('error', {}).get('message', 'Query execution failed')\n",
    "            error_msg = f\"Query failed: {error_detail}\"\n",
    "            print(f\"[Genie] {error_msg}\")\n",
    "            return {\"error\": error_msg}\n",
    "        \n",
    "        if poll_status != 'completed':\n",
    "            error_msg = f\"Query did not complete: {poll_result.get('error', poll_status)}\"\n",
    "            print(f\"[Genie] {error_msg}\")\n",
    "            return {\"error\": error_msg}\n",
    "        \n",
    "        # Step 3: Extract results from poll response\n",
    "        response = poll_result['response']\n",
    "        text_content = response.get('content', '')\n",
    "        attachments = response.get('attachments', [])\n",
    "        \n",
    "        print(f\"[Genie] Received response with {len(attachments)} attachments\")\n",
    "        \n",
    "        result = {\n",
    "            \"text\": text_content,\n",
    "            \"query\": None,\n",
    "            \"data\": None,\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"message_id\": message_id\n",
    "        }\n",
    "        \n",
    "        # Step 4: Extract SQL query and attachment_id\n",
    "        if attachments:\n",
    "            attachment = attachments[0]\n",
    "            \n",
    "            # CRITICAL: Field is 'attachment_id', NOT 'id'!\n",
    "            attachment_id = attachment.get('attachment_id') or attachment.get('id')\n",
    "            print(f\"[Genie] Attachment ID: {attachment_id}\")\n",
    "            \n",
    "            # Extract SQL query\n",
    "            query_obj = attachment.get('query', {})\n",
    "            if isinstance(query_obj, dict):\n",
    "                result['query'] = query_obj.get('query') or query_obj.get('content')\n",
    "                if result['query']:\n",
    "                    print(f\"[Genie] Extracted SQL: {result['query'][:100]}...\")\n",
    "            \n",
    "            # Step 5: Fetch actual data using query-result endpoint\n",
    "            if attachment_id:\n",
    "                print(f\"[Genie] Calling query-result endpoint...\")\n",
    "                try:\n",
    "                    query_result_response = self.w.api_client.do(\n",
    "                        'GET',\n",
    "                        f'/api/2.0/genie/spaces/{self.space_id}/conversations/{conversation_id}/messages/{message_id}/query-result/{attachment_id}'\n",
    "                    )\n",
    "                    \n",
    "                    # Parse response - data is wrapped in 'statement_response'\n",
    "                    statement_response = query_result_response.get('statement_response', {})\n",
    "                    if statement_response:\n",
    "                        print(f\"[Genie] Found statement_response\")\n",
    "                        manifest = statement_response.get('manifest', {})\n",
    "                        if manifest:\n",
    "                            schema = manifest.get('schema', {})\n",
    "                            columns = schema.get('columns', [])\n",
    "                            column_names = [col.get('name') for col in columns]\n",
    "                            print(f\"[Genie] Found columns: {column_names}\")\n",
    "                            \n",
    "                            # Get data rows\n",
    "                            result_obj = statement_response.get('result', {})\n",
    "                            data_array = result_obj.get('data_array', [])\n",
    "                            print(f\"[Genie] Found {len(data_array)} data rows from Genie API\")\n",
    "                            \n",
    "                            if data_array:\n",
    "                                # Convert to list of dicts\n",
    "                                result['data'] = []\n",
    "                                for row in data_array:\n",
    "                                    row_dict = dict(zip(column_names, row))\n",
    "                                    result['data'].append(row_dict)\n",
    "                                print(f\"[Genie] ‚úÖ Successfully converted {len(result['data'])} rows\")\n",
    "                            else:\n",
    "                                print(f\"[Genie] No data_array in result\")\n",
    "                        else:\n",
    "                            print(f\"[Genie] No manifest in statement_response\")\n",
    "                    else:\n",
    "                        print(f\"[Genie] No statement_response in query result response\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"[Genie] Error calling query-result endpoint: {str(e)}\")\n",
    "        \n",
    "        # Step 6: FALLBACK - Execute SQL directly if we have query but no data\n",
    "        if result.get('query') and not result.get('data'):\n",
    "            print(f\"[Genie] FALLBACK: No data from Genie API, executing SQL directly...\")\n",
    "            result['used_fallback'] = True\n",
    "            try:\n",
    "                from databricks.sdk.service.sql import StatementState\n",
    "                \n",
    "                execute_response = self.w.statement_execution.execute_statement(\n",
    "                    warehouse_id=WAREHOUSE_ID,\n",
    "                    statement=result['query'],\n",
    "                    wait_timeout='30s'\n",
    "                )\n",
    "                \n",
    "                print(f\"[Genie] Fallback execution status: {execute_response.status.state}\")\n",
    "                \n",
    "                if execute_response.status.state == StatementState.SUCCEEDED:\n",
    "                    columns = execute_response.manifest.schema.columns if execute_response.manifest and execute_response.manifest.schema else []\n",
    "                    column_names = [col.name for col in columns]\n",
    "                    print(f\"[Genie] Fallback found columns: {column_names}\")\n",
    "                    \n",
    "                    if execute_response.result and execute_response.result.data_array:\n",
    "                        data_array = execute_response.result.data_array\n",
    "                        print(f\"[Genie] Fallback found {len(data_array)} data rows\")\n",
    "                        \n",
    "                        result['data'] = []\n",
    "                        for row in data_array:\n",
    "                            row_dict = dict(zip(column_names, row))\n",
    "                            result['data'].append(row_dict)\n",
    "                        print(f\"[Genie] ‚úÖ Fallback successfully converted {len(result['data'])} rows\")\n",
    "                else:\n",
    "                    error_msg = execute_response.status.error.message if execute_response.status.error else \"Unknown error\"\n",
    "                    print(f\"[Genie] Fallback execution failed: {error_msg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Genie] Fallback execution error: {str(e)}\")\n",
    "        \n",
    "        print(f\"[Genie] Final data rows extracted: {len(result.get('data') or [])}\")\n",
    "        \n",
    "        # Determine which method was used\n",
    "        if result.get('data'):\n",
    "            if result.get('used_fallback'):\n",
    "                result['method'] = 'Direct SQL Execution (Fallback)'\n",
    "            else:\n",
    "                result['method'] = 'Genie query-result API'\n",
    "            print(f\"[Genie] Method: {result['method']}\")\n",
    "        \n",
    "        print(f\"[Genie] === Genie Query Complete ===\\n\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2f9cf5-5050-44fe-a377-4c762b3ddec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 4: Genie Conversation API\n",
      "================================================================================\n",
      "[Genie] Initialized with space_id: 01f0d3b893281964829ca73bcfa4d7c7\n",
      "\n",
      "[Genie] === Starting Genie Query ===\n",
      "[Genie] Question: Show me 3 recent resolved tickets about database connection issues...\n",
      "[Genie] Starting conversation...\n",
      "[Genie] ‚úÖ Started - conversation_id: 01f0d4fb38b91f769588d0316706c023, message_id: 01f0d4fb38c815029d28411cc7395431\n",
      "[Genie] Polling for result (max 120s)...\n",
      "[Genie] Poll #1 (0.1s): Status = FETCHING_METADATA\n",
      "[Genie] Poll #2 (2.2s): Status = ASKING_AI\n",
      "[Genie] Poll #3 (4.7s): Status = PENDING_WAREHOUSE\n",
      "[Genie] Poll #4 (7.7s): Status = ASKING_AI\n",
      "[Genie] Poll #5 (11.3s): Status = PENDING_WAREHOUSE\n",
      "[Genie] Poll #6 (15.6s): Status = COMPLETED\n",
      "[Genie] ‚úÖ Query completed!\n",
      "[Genie] Received response with 3 attachments\n",
      "[Genie] Attachment ID: 01f0d4fb3e0c127580612a5d5286e3f6\n",
      "[Genie] Extracted SQL: SELECT `ticket_id`, `subject`, `description`, `submitted_date`, `expected_category`, `expected_prior...\n",
      "[Genie] Calling query-result endpoint...\n",
      "[Genie] Found statement_response\n",
      "[Genie] Found columns: ['ticket_id', 'subject', 'description', 'submitted_date', 'expected_category', 'expected_priority', 'expected_team']\n",
      "[Genie] Found 2 data rows from Genie API\n",
      "[Genie] ‚úÖ Successfully converted 2 rows\n",
      "[Genie] Final data rows extracted: 2\n",
      "[Genie] Method: Genie query-result API\n",
      "[Genie] === Genie Query Complete ===\n",
      "\n",
      "\n",
      "ü§ñ Genie Result:\n",
      "  üìù Text Response: Show me 3 recent resolved tickets about database connection issues...\n",
      "  üîç Generated SQL: SELECT `ticket_id`, `subject`, `description`, `submitted_date`, `expected_category`, `expected_priority`, `expected_team` FROM `langraph_demo`.`agents`.`sample_tickets` WHERE ( (`subject` ILIKE '%data...\n",
      "  üìä Data Rows: 2\n",
      "  üîß Method: Genie query-result API\n",
      "\n",
      "  Sample rows:\n",
      "    1. {'ticket_id': 'TKT-004', 'subject': 'VPN Disconnecting Every 15 Minutes - Sales Team Affected', 'description': 'Our entire sales team (about 30 people) is experiencing VPN disconnections every 15-20 minutes. They need stable VPN to access CRM and sales databases. This started this morning around 9 AM EST. Getting \"Connection lost - reconnecting\" errors. VPN server is VPN-GATEWAY-02. Cisco AnyConnect version 4.10.', 'submitted_date': '2025-12-04T08:29:03.852-05:00', 'expected_category': 'Infrastructure', 'expected_priority': 'P2', 'expected_team': 'Infrastructure'}\n",
      "    2. {'ticket_id': 'TKT-001', 'subject': 'URGENT: Production Database Server Down', 'description': 'Our main production database server PROD-DB-01 is completely down. All customer-facing applications are showing error messages. This is affecting hundreds of users and we\\'re losing revenue. Database logs show \"Connection refused\" errors. This started 5 minutes ago and needs immediate attention!', 'submitted_date': '2025-12-04T02:58:03.852-05:00', 'expected_category': 'Infrastructure', 'expected_priority': 'P1', 'expected_team': 'Infrastructure'}\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Genie API\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 4: Genie Conversation API\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "genie = GenieConversationTool(w, GENIE_SPACE_ID)\n",
    "genie_question = \"Show me 3 recent resolved claims about database connection issues\"\n",
    "genie_result = genie.query(genie_question)\n",
    "\n",
    "print(f\"\\nü§ñ Genie Result:\")\n",
    "print(f\"  üìù Text Response: {genie_result.get('text', '')[:300]}...\")\n",
    "if genie_result.get('query'):\n",
    "    print(f\"  üîç Generated SQL: {genie_result['query'][:200]}...\")\n",
    "if genie_result.get('data'):\n",
    "    print(f\"  üìä Data Rows: {len(genie_result['data'])}\")\n",
    "    print(f\"  üîß Method: {genie_result.get('method', 'Unknown')}\")\n",
    "    print(f\"\\n  Sample rows:\")\n",
    "    for i, row in enumerate(genie_result['data'][:3], 1):\n",
    "        print(f\"    {i}. {row}\")\n",
    "elif genie_result.get('error'):\n",
    "    print(f\"  ‚ùå Error: {genie_result['error']}\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è No data returned (query generated but data not fetched)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae28c210-ce77-43fd-a08c-529444d91b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 2: Create LangChain Tools\n",
    "\n",
    "Wrap the working functions as LangChain Tools for the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b337aa-38e1-409b-99e1-1522b466e904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß∞ Wrap Functions as LangChain Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e81646-576c-4dc2-80d8-c28f649631a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING LANGCHAIN TOOLS\n",
      "================================================================================\n",
      "‚úÖ Tool 1: classify_ticket\n",
      "‚úÖ Tool 2: extract_metadata\n",
      "‚úÖ Tool 3: search_knowledge\n",
      "‚úÖ Tool 4: query_historical\n",
      "\n",
      "üéâ All 4 LangChain Tools created successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CREATING LANGCHAIN TOOLS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define Pydantic schemas for tool inputs (fixes LLM parameter naming issues)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ClassifyClaimInput(BaseModel):\n",
    "    claim_text: str = Field(description=\"The support claim text to classify\")\n",
    "\n",
    "class ExtractMetadataInput(BaseModel):\n",
    "    claim_text: str = Field(description=\"The support claim text to extract metadata from\")\n",
    "\n",
    "class SearchKnowledgeInput(BaseModel):\n",
    "    query: str = Field(description=\"The search query to find relevant documentation\")\n",
    "\n",
    "class QueryHistoricalInput(BaseModel):\n",
    "    question: str = Field(description=\"Natural language question about historical claims\")\n",
    "\n",
    "# Tool 1: Classification\n",
    "def classify_claim_wrapper(claim_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrapper for fraud_classify UC Function.\n",
    "    \n",
    "    IMPORTANT: This function MUST be pure (no side effects)!\n",
    "    - ‚ùå NO Streamlit calls (st.info, st.error, etc.) - causes NoSessionContext error\n",
    "    - ‚úÖ Return JSON string only\n",
    "    - ‚úÖ Include error handling in return value\n",
    "    \n",
    "    WHY: LangGraph runs tools in background threads without Streamlit session context\n",
    "    \"\"\"\n",
    "    result = call_uc_function(\"fraud_classify\", {\"claim_text\": claim_text})\n",
    "    return json.dumps(result, indent=2) if result else json.dumps({\"error\": \"Classification failed\"})\n",
    "\n",
    "classify_tool = Tool(\n",
    "    name=\"classify_claim\",\n",
    "    description=\"Classifies a support claim into category, priority, and routing team. Use this FIRST to understand the claim type. Returns JSON with category, priority, team, confidence.\",\n",
    "    func=classify_claim_wrapper,\n",
    "    args_schema=ClassifyClaimInput\n",
    ")\n",
    "print(\"‚úÖ Tool 1: classify_claim\")\n",
    "\n",
    "# Tool 2: Metadata Extraction\n",
    "def extract_metadata_wrapper(claim_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrapper for fraud_extract_indicators UC Function.\n",
    "    IMPORTANT: Pure function - no Streamlit calls! (See classify_claim_wrapper for details)\n",
    "    \"\"\"\n",
    "    result = call_uc_function(\"fraud_extract_indicators\", {\"claim_text\": claim_text})\n",
    "    return json.dumps(result, indent=2) if result else json.dumps({\"error\": \"Extraction failed\"})\n",
    "\n",
    "extract_tool = Tool(\n",
    "    name=\"extract_metadata\",\n",
    "    description=\"Extracts structured metadata from claim including risk score, urgency indicators, red flags, and technical details. Use for complex suspicious claims. Returns JSON with priority_score, urgency, systems, details.\",\n",
    "    func=extract_metadata_wrapper,\n",
    "    args_schema=ExtractMetadataInput\n",
    ")\n",
    "print(\"‚úÖ Tool 2: extract_metadata\")\n",
    "\n",
    "# Tool 3: Knowledge Base Search\n",
    "def search_knowledge_wrapper(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrapper for Vector Search.\n",
    "    IMPORTANT: Pure function - no Streamlit calls! (See classify_claim_wrapper for details)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = search_knowledge_base(query, num_results=3)\n",
    "        return json.dumps(results, indent=2) if results else json.dumps([])\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Search failed: {str(e)}\"})\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"search_knowledge\",\n",
    "    description=\"Searches the fraud knowledge base for relevant articles, documentation, and solutions using semantic search. Use to find how-to guides, troubleshooting steps, or existing documentation. Returns JSON array with title, content, category for top matches.\",\n",
    "    func=search_knowledge_wrapper,\n",
    "    args_schema=SearchKnowledgeInput\n",
    ")\n",
    "print(\"‚úÖ Tool 3: search_knowledge\")\n",
    "\n",
    "# Tool 4: Historical Claims (Genie)\n",
    "def query_historical_wrapper(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrapper for Genie Conversation API.\n",
    "    IMPORTANT: Pure function - no Streamlit calls! (See classify_claim_wrapper for details)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        genie = GenieConversationTool(w, GENIE_SPACE_ID)\n",
    "        result = genie.query(question)\n",
    "        return json.dumps(result, indent=2, default=str)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Historical query failed: {str(e)}\"})\n",
    "\n",
    "genie_tool = Tool(\n",
    "    name=\"query_historical\",\n",
    "    description=\"Queries historical resolved claims using natural language to find similar fraud cases and their resolutions. Use for complex issues where past solutions might help. Returns JSON with text summary and optionally SQL query used.\",\n",
    "    func=query_historical_wrapper,\n",
    "    args_schema=QueryHistoricalInput\n",
    ")\n",
    "print(\"‚úÖ Tool 4: query_historical\")\n",
    "\n",
    "print(f\"\\nüéâ All 4 LangChain Tools created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c538cbe-00ea-4436-bac6-f3b045197285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 3: Create LangGraph ReAct Agent\n",
    "\n",
    "Build the agent that will intelligently decide which tools to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f316299-d8b2-4172-ab15-06255a930287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Initialize LLM and Create Agent\n",
    "\n",
    "**Critical LangGraph v1.0+ Pattern:**\n",
    "This is the **main fix** that made the agent work!\n",
    "\n",
    "**What Changed in LangGraph v1.0:**\n",
    "- ‚ùå Old: `state_modifier` parameter (removed in v1.0)\n",
    "- ‚úÖ New: Pass SystemMessage in messages array at invocation time\n",
    "\n",
    "**Why This Approach:**\n",
    "1. **Compatible** - Works with LangGraph v1.0+\n",
    "2. **Explicit** - Clear what messages are sent to LLM\n",
    "3. **Flexible** - Can change system prompt per invocation if needed\n",
    "4. **Simple** - No complex state management required\n",
    "\n",
    "**Alternative Approaches (for reference):**\n",
    "- LangGraph v0.2: Used `state_modifier` parameter\n",
    "- LangGraph v1.0 advanced: Use custom `StateGraph` with preprocessor\n",
    "- LangChain v1.0: Use `langchain.claims_analysis.create_agent()` with state_schema\n",
    "\n",
    "We chose the simplest approach that works across versions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ac0fef-ba51-417c-8e7b-8ac620f04c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING LANGGRAPH REACT AGENT\n",
      "================================================================================\n",
      "‚úÖ LLM initialized: databricks-claude-sonnet-4\n",
      "‚úÖ Tools bound to LLM (ensures proper JSON format)\n",
      "‚úÖ LangGraph ReAct Agent created!\n",
      "üß∞ Agent has 4 tools available:\n",
      "   1. classify_ticket\n",
      "   2. extract_metadata\n",
      "   3. search_knowledge\n",
      "   4. query_historical\n",
      "\n",
      "üéØ Agent will decide which tools to use based on the ticket!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-9c1cfb39-e40b-4c60-8e61-14/.ipykernel/41127/command-6160301153452420-3734944829:48: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CREATING LANGGRAPH REACT AGENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# System prompt for the agent\n",
    "# This guides the agent's decision-making process\n",
    "# Key insight: Agent should be EFFICIENT, not exhaustive\n",
    "system_prompt = \"\"\"You are an intelligent support claim analysis assistant.\n",
    "\n",
    "Your goal is to efficiently gather the RIGHT information to help resolve \n",
    "the claim - not to blindly call every tool available.\n",
    "\n",
    "GUIDELINES:\n",
    "1. ALWAYS classify the claim first to understand its nature\n",
    "2. For simple how-to questions, the fraud knowledge base is usually sufficient\n",
    "3. For critical production issues, check historical claims for proven solutions\n",
    "4. For low-priority simple questions, metadata extraction is usually not needed\n",
    "5. Only use query_historical for complex or critical issues where past patterns matter\n",
    "6. Be efficient but thorough - only use tools that add value\n",
    "\n",
    "Think step-by-step, explain your reasoning, and make smart decisions about which tools to use.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize LLM\n",
    "# ChatDatabricks is a LangChain wrapper for Databricks Foundation Models\n",
    "# It handles authentication via WorkspaceClient automatically\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=LLM_ENDPOINT,  # Foundation model serving endpoint\n",
    "    temperature=0.1,  # VERY LOW temp for reliable tool calling (was 0.3, reduced to 0.1)\n",
    "    max_tokens=4096  # Sufficient for reasoning + tool calls\n",
    ")\n",
    "print(f\"‚úÖ LLM initialized: {LLM_ENDPOINT}\")\n",
    "\n",
    "# Create ReAct agent with all 4 tools\n",
    "# ReAct = Reasoning + Acting (agent thinks before acting)\n",
    "# Pattern: Thought ‚Üí Action (tool call) ‚Üí Observation (tool result) ‚Üí repeat\n",
    "tools_list = [classify_tool, extract_tool, search_tool, genie_tool]\n",
    "\n",
    "# üö® CRITICAL FIX: Use bind_tools for reliable function calling\n",
    "# This ensures the LLM uses proper JSON format (not XML) for ALL tool calls\n",
    "# Without this, Claude might hallucinate XML format mid-conversation\n",
    "llm_with_tools = llm.bind_tools(tools_list)\n",
    "print(f\"‚úÖ Tools bound to LLM (ensures proper JSON format)\")\n",
    "\n",
    "# Create the agent using the standard create_react_agent\n",
    "# IMPORTANT: Pass the bound LLM (llm_with_tools) not the raw LLM\n",
    "# This ensures consistent tool calling format throughout the conversation\n",
    "agent = create_react_agent(\n",
    "    model=llm_with_tools,  # The LLM with tools bound (CRITICAL!)\n",
    "    tools=tools_list  # Available tools the agent can call\n",
    "    # NOTE: We do NOT pass state_modifier here (deprecated in v1.0)\n",
    "    # Instead, we'll inject SystemMessage when we invoke the agent\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LangGraph ReAct Agent created!\")\n",
    "print(f\"üß∞ Agent has 4 tools available:\")\n",
    "print(f\"   1. classify_claim\")\n",
    "print(f\"   2. extract_metadata\")\n",
    "print(f\"   3. search_knowledge\")\n",
    "print(f\"   4. query_historical\")\n",
    "print(f\"\\nüéØ Agent will decide which tools to use based on the claim!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4503ddf1-027f-4d48-b368-b56f149aa04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 4: Test Agent with Different Claims\n",
    "\n",
    "See how the agent adapts to different claim complexities\n",
    "\n",
    "## üîÑ How the ReAct Pattern Works\n",
    "\n",
    "![ReAct Pattern Execution](/Workspace/Users/vik.malhotra@databricks.com/.bundle/langraph-tutorial/test/files/docs/images/flowchart_showing_react_claim.png)\n",
    "\n",
    "*The agent loops through Reason ‚Üí Act ‚Üí Observe ‚Üí Decide until it has enough information to provide a complete answer*\n",
    "\n",
    "**Key Points:**\n",
    "- Simple claims use 2-3 tools (fast, cheap)\n",
    "- Complex claims use all 4 tools (thorough, accurate)\n",
    "- Agent decides adaptively based on claim content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6538e133-ebf3-48eb-a54b-b41c416402e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß™ Test Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e1417f-796f-47ba-9aed-c35ed2295919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_agent_test(claim_text: str, show_reasoning: bool = True):\n",
    "    \"\"\"\n",
    "    Test agent with a claim and display results\n",
    "    \n",
    "    This function demonstrates the LangGraph v1.0+ invocation pattern\n",
    "    \n",
    "    **Key Pattern: Manual SystemMessage Injection**\n",
    "    We pass the system prompt as the FIRST message when invoking the agent.\n",
    "    This is the v1.0+ replacement for the deprecated state_modifier parameter.\n",
    "    \n",
    "    Args:\n",
    "        claim_text: The support claim to analyze\n",
    "        show_reasoning: Whether to display the agent's thought process\n",
    "        \n",
    "    Returns:\n",
    "        Agent result containing messages and tool calls\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üé´ TICKET: {claim_text}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # CRITICAL PATTERN: Manual SystemMessage Injection (v1.0+)\n",
    "    # We explicitly pass SystemMessage as first message\n",
    "    # This replaces the old state_modifier approach from v0.2\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [\n",
    "            # System message goes FIRST - sets agent behavior/personality\n",
    "            SystemMessage(content=system_prompt),\n",
    "            # Then user message with the actual task\n",
    "            (\"user\", f\"Analyze this support claim and provide recommendations: {claim_text}\")\n",
    "        ]\n",
    "    })\n",
    "    # The agent will:\n",
    "    # 1. Read system prompt (guidance)\n",
    "    # 2. Read user message (task)\n",
    "    # 3. Reason about what tools to call\n",
    "    # 4. Call tools as needed\n",
    "    # 5. Synthesize final answer\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if show_reasoning:\n",
    "        print(\"üß† AGENT REASONING TRAIL:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, message in enumerate(result['messages']):\n",
    "            msg_type = getattr(message, 'type', None)\n",
    "            \n",
    "            if msg_type == \"human\":\n",
    "                print(f\"\\nüë§ USER:\")\n",
    "                content = getattr(message, 'content', '')\n",
    "                print(f\"   {str(content)[:200]}...\")\n",
    "            elif msg_type == \"ai\":\n",
    "                content = getattr(message, 'content', '') or \"\"\n",
    "                # Check if this is a tool call or final answer\n",
    "                tool_calls = getattr(message, 'tool_calls', None)\n",
    "                if tool_calls:\n",
    "                    print(f\"\\nü§ñ AGENT THOUGHT & ACTION:\")\n",
    "                    if content:\n",
    "                        print(f\"   {str(content)[:300]}...\")\n",
    "                    for tool_call in tool_calls:\n",
    "                        tool_name = tool_call.get('name') if isinstance(tool_call, dict) else getattr(tool_call, 'name', 'unknown')\n",
    "                        tool_args = tool_call.get('args') if isinstance(tool_call, dict) else getattr(tool_call, 'args', {})\n",
    "                        print(f\"   üîß Calling tool: {tool_name}\")\n",
    "                        print(f\"   üì• Input: {str(tool_args)[:150]}...\")\n",
    "                else:\n",
    "                    print(f\"\\nü§ñ AGENT FINAL ANSWER:\")\n",
    "                    if content:\n",
    "                        print(f\"   {str(content)[:500]}...\")\n",
    "                    else:\n",
    "                        print(f\"   (No answer generated)\")\n",
    "            elif msg_type == \"tool\":  # Tool message\n",
    "                tool_name = getattr(message, 'name', 'unknown')\n",
    "                print(f\"\\nüì§ TOOL RESULT ({tool_name}):\")\n",
    "                content = getattr(message, 'content', None)\n",
    "                if content:\n",
    "                    result_preview = str(content)[:200] if len(str(content)) > 200 else str(content)\n",
    "                    print(f\"   {result_preview}...\")\n",
    "                else:\n",
    "                    print(f\"   (No content)\")\n",
    "    \n",
    "    # Count tools used\n",
    "    tool_messages = [m for m in result['messages'] if getattr(m, 'type', None) == 'tool']\n",
    "    tools_used = list(set([getattr(m, 'name', None) for m in tool_messages if getattr(m, 'name', None)]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìä SUMMARY:\")\n",
    "    print(f\"   ‚è±Ô∏è  Time: {elapsed_time:.2f}s\")\n",
    "    if tools_used:\n",
    "        print(f\"   üß∞ Tools used: {len(tools_used)}/4 - {', '.join(tools_used)}\")\n",
    "    else:\n",
    "        print(f\"   üß∞ Tools used: 0/4\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4ad85a-c3f2-4287-8ac7-07b3bb55cfec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß™ Test 1: Simple Question (Expected: 2 tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96f0d40-6c3b-400e-8dc1-dccb33ef56e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 1: SIMPLE QUESTION\n",
      "Expected behavior: Should use classify + search, skip extract and genie\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üé´ TICKET: How do I reset my password?\n",
      "================================================================================\n",
      "\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_classify\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_classify('How do I reset my password?') as result...\n",
      "‚úÖ UC Function result received\n",
      "üîç Searching knowledge base: 'password reset how to guide...'\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "‚úÖ Found 3 documents\n",
      "üß† AGENT REASONING TRAIL:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ USER:\n",
      "   Analyze this support ticket and provide recommendations: How do I reset my password?...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   I'll analyze this support ticket step by step. Let me start by classifying it to understand its nature, then determine what additional information might be helpful....\n",
      "   üîß Calling tool: classify_ticket\n",
      "   üì• Input: {'ticket_text': 'How do I reset my password?'}...\n",
      "\n",
      "üì§ TOOL RESULT (classify_ticket):\n",
      "   {\n",
      "  \"category\": \"Software\",\n",
      "  \"priority\": \"P3\",\n",
      "  \"assigned_team\": \"Applications\",\n",
      "  \"confidence\": \"0.0\"\n",
      "}...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   Now let me search the knowledge base for password reset documentation, as this appears to be a straightforward how-to question that should have existing documentation....\n",
      "   üîß Calling tool: search_knowledge\n",
      "   üì• Input: {'query': 'password reset how to guide'}...\n",
      "\n",
      "üì§ TOOL RESULT (search_knowledge):\n",
      "   [\n",
      "  {\n",
      "    \"doc_id\": \"Policy_USER_ACCESS_POLICIES_006\",\n",
      "    \"doc_type\": \"Policy\",\n",
      "    \"title\": \"USER ACCESS MANAGEMENT POLICIES - SECTION 4: PASSWORD MANAGEMENT (Part 6)\",\n",
      "    \"content\": \"-------------...\n",
      "\n",
      "ü§ñ AGENT FINAL ANSWER:\n",
      "   ## Analysis and Recommendations\n",
      "\n",
      "Based on my analysis, here's what I found:\n",
      "\n",
      "### Ticket Classification\n",
      "- **Category**: Software\n",
      "- **Priority**: P3 (Standard priority)\n",
      "- **Assigned Team**: Applications\n",
      "- **Type**: Simple how-to/service request\n",
      "\n",
      "### Recommendations\n",
      "\n",
      "**1. Immediate Action:**\n",
      "This is a straightforward password reset request that can be resolved quickly using existing self-service options.\n",
      "\n",
      "**2. Resolution Path:**\n",
      "According to the knowledge base, the user should:\n",
      "- Use the **password...\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   ‚è±Ô∏è  Time: 11.41s\n",
      "   üß∞ Tools used: 2/4 - search_knowledge, classify_ticket\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[{\"trace_id\": \"tr-307baebec1786323b993144b4633fc30\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-9ac26a5780caf13f459093ea739b6b20\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-9e771de448ecac20893ebb2c7791ee1d\", \"sql_warehouse_id\": null}]",
      "text/plain": [
       "[Trace(trace_id=tr-307baebec1786323b993144b4633fc30), Trace(trace_id=tr-9ac26a5780caf13f459093ea739b6b20), Trace(trace_id=tr-9e771de448ecac20893ebb2c7791ee1d)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"üß™ TEST 1: SIMPLE QUESTION\")\n",
    "print(\"Expected behavior: Should use classify + search, skip extract and genie\\n\")\n",
    "\n",
    "result_1 = run_agent_test(\"How do I reset my password?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3341ee-24a9-4287-abbc-d0133d60ebe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß™ Test 2: Critical Production Issue (Expected: 3-4 tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1511ed26-6afc-415a-b48c-b57e352adc22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 2: CRITICAL PRODUCTION ISSUE\n",
      "Expected behavior: Should use classify + search + genie, possibly extract\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üé´ TICKET: Production database connection timeout affecting all users, urgent!\n",
      "================================================================================\n",
      "\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_classify\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_classify('Production database connection timeout affecting all users,...\n",
      "‚úÖ UC Function result received\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_extract\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_extract('Production database connection timeout affecting all users, ...\n",
      "‚úÖ UC Function result received\n",
      "üîç Searching knowledge base: 'database connection timeout production troubleshoo...'\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "‚úÖ Found 3 documents\n",
      "[Genie] Initialized with space_id: 01f0d3b893281964829ca73bcfa4d7c7\n",
      "\n",
      "[Genie] === Starting Genie Query ===\n",
      "[Genie] Question: How were previous production database connection timeout issues affecting all users resolved?...\n",
      "[Genie] Starting conversation...\n",
      "[Genie] ‚úÖ Started - conversation_id: 01f0d4fb51c21adb914882da935e07f2, message_id: 01f0d4fb51cb1f4cba0e4a53abfc6dbf\n",
      "[Genie] Polling for result (max 120s)...\n",
      "[Genie] Poll #1 (0.1s): Status = FETCHING_METADATA\n",
      "[Genie] Poll #2 (2.2s): Status = ASKING_AI\n",
      "[Genie] Poll #3 (4.7s): Status = ASKING_AI\n",
      "[Genie] Poll #4 (7.7s): Status = ASKING_AI\n",
      "[Genie] Poll #5 (11.2s): Status = ASKING_AI\n",
      "[Genie] Poll #6 (15.5s): Status = ASKING_AI\n",
      "[Genie] Poll #7 (20.6s): Status = COMPLETED\n",
      "[Genie] ‚úÖ Query completed!\n",
      "[Genie] Received response with 2 attachments\n",
      "[Genie] Attachment ID: 01f0d4fb5c021800a1c745dbdbaa4ec7\n",
      "[Genie] Extracted SQL: SELECT `ticket_id`, `subject`, `description`, `ticket_text` FROM `langraph_demo`.`agents`.`sample_ti...\n",
      "[Genie] Calling query-result endpoint...\n",
      "[Genie] Found statement_response\n",
      "[Genie] Found columns: ['ticket_id', 'subject', 'description', 'ticket_text']\n",
      "[Genie] Found 0 data rows from Genie API\n",
      "[Genie] No data_array in result\n",
      "[Genie] FALLBACK: No data from Genie API, executing SQL directly...\n",
      "[Genie] Fallback execution status: StatementState.SUCCEEDED\n",
      "[Genie] Fallback found columns: ['ticket_id', 'subject', 'description', 'ticket_text']\n",
      "[Genie] Final data rows extracted: 0\n",
      "[Genie] === Genie Query Complete ===\n",
      "\n",
      "üß† AGENT REASONING TRAIL:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ USER:\n",
      "   Analyze this support ticket and provide recommendations: Production database connection timeout affecting all users, urgent!...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   I'll analyze this support ticket systematically to provide the best recommendations. Let me start by classifying it to understand the ticket type and urgency....\n",
      "   üîß Calling tool: classify_ticket\n",
      "   üì• Input: {'ticket_text': 'Production database connection timeout affecting all users, urgent!'}...\n",
      "\n",
      "üì§ TOOL RESULT (classify_ticket):\n",
      "   {\n",
      "  \"category\": \"Software\",\n",
      "  \"priority\": \"P3\",\n",
      "  \"assigned_team\": \"Applications\",\n",
      "  \"confidence\": \"0.0\"\n",
      "}...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   The classification shows this as a software issue, but I notice the priority is marked as P3 despite the \"urgent\" nature and \"affecting all users\" - this seems like it should be higher priority. Let me extract more detailed metadata to better understand the severity and technical details....\n",
      "   üîß Calling tool: extract_metadata\n",
      "   üì• Input: {'ticket_text': 'Production database connection timeout affecting all users, urgent!'}...\n",
      "\n",
      "üì§ TOOL RESULT (extract_metadata):\n",
      "   {\n",
      "  \"requires_escalation\": \"false\",\n",
      "  \"priority_score\": \"0.5\",\n",
      "  \"technical_keywords\": [],\n",
      "  \"urgency_level\": \"Medium\",\n",
      "  \"affected_systems\": [],\n",
      "  \"user_impact\": \"Single User\"\n",
      "}...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   The metadata extraction seems to underestimate the severity (showing \"Single User\" when the ticket clearly states \"all users\"). This is a critical production database issue affecting all users, so I need to search for relevant solutions and check historical patterns for similar database timeout issu...\n",
      "   üîß Calling tool: search_knowledge\n",
      "   üì• Input: {'query': 'database connection timeout production troubleshooting steps'}...\n",
      "\n",
      "üì§ TOOL RESULT (search_knowledge):\n",
      "   [\n",
      "  {\n",
      "    \"doc_id\": \"Guide_DATABASE_ADMIN_GUIDE_002\",\n",
      "    \"doc_type\": \"Guide\",\n",
      "    \"title\": \"DATABASE ADMINISTRATION GUIDE - DATABASE CONNECTION ISSUES (Part 2)\",\n",
      "    \"content\": \"---------------------...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   Great! I found relevant documentation. Now let me check historical tickets to see how similar database timeout issues were resolved in the past....\n",
      "   üîß Calling tool: query_historical\n",
      "   üì• Input: {'question': 'How were previous production database connection timeout issues affecting all users resolved?'}...\n",
      "\n",
      "üì§ TOOL RESULT (query_historical):\n",
      "   {\n",
      "  \"text\": \"How were previous production database connection timeout issues affecting all users resolved?\",\n",
      "  \"query\": \"SELECT `ticket_id`, `subject`, `description`, `ticket_text` FROM `langraph_demo...\n",
      "\n",
      "ü§ñ AGENT FINAL ANSWER:\n",
      "   ## Analysis and Recommendations\n",
      "\n",
      "Based on my analysis, this is a **critical production issue** that requires immediate attention. Here's my assessment:\n",
      "\n",
      "### Severity Assessment\n",
      "- **Actual Priority**: This should be **P1 Critical** (not P3 as initially classified)\n",
      "- **Impact**: All users affected - complete service disruption\n",
      "- **System**: Production database - core infrastructure component\n",
      "- **Urgency**: Immediate action required\n",
      "\n",
      "### Immediate Action Plan\n",
      "\n",
      "From the knowledge base documentation,...\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   ‚è±Ô∏è  Time: 45.36s\n",
      "   üß∞ Tools used: 4/4 - query_historical, extract_metadata, search_knowledge, classify_ticket\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[{\"trace_id\": \"tr-8ed7391a451f098f7038601a27b97161\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-8d10fb92614720f1164317b1ddab515c\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-30e4aeea5d50592174b499a6275cfbd5\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-f2fc1dfac058f00f7946b9c814c03ad9\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-de879faf85c26dd7ea5cca49bd56eebe\", \"sql_warehouse_id\": null}]",
      "text/plain": [
       "[Trace(trace_id=tr-8ed7391a451f098f7038601a27b97161), Trace(trace_id=tr-8d10fb92614720f1164317b1ddab515c), Trace(trace_id=tr-30e4aeea5d50592174b499a6275cfbd5), Trace(trace_id=tr-f2fc1dfac058f00f7946b9c814c03ad9), Trace(trace_id=tr-de879faf85c26dd7ea5cca49bd56eebe)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"üß™ TEST 2: CRITICAL PRODUCTION ISSUE\")\n",
    "print(\"Expected behavior: Should use classify + search + genie, possibly extract\\n\")\n",
    "\n",
    "result_2 = run_agent_test(\"Production database connection timeout affecting all users, urgent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb06567b-341a-45ab-ad8d-df6ebb4ddcb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß™ Test 3: Feature Request (Expected: 2-3 tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852b8198-12b8-4a51-9c40-01c56d426937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 3: FEATURE REQUEST\n",
      "Expected behavior: Should use classify + extract, maybe search\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üé´ TICKET: Need to integrate our system with Salesforce API for syncing customer data\n",
      "================================================================================\n",
      "\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_classify\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_classify('Need to integrate our system with Salesforce API for syncin...\n",
      "‚úÖ UC Function result received\n",
      "üîç Searching knowledge base: 'Salesforce API integration customer data sync...'\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "‚úÖ Found 3 documents\n",
      "üîç Searching knowledge base: 'API integration third-party system data synchroniz...'\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "‚úÖ Found 3 documents\n",
      "[Genie] Initialized with space_id: 01f0d3b893281964829ca73bcfa4d7c7\n",
      "\n",
      "[Genie] === Starting Genie Query ===\n",
      "[Genie] Question: What are common solutions and approaches for Salesforce API integration projects for customer data s...\n",
      "[Genie] Starting conversation...\n",
      "[Genie] ‚úÖ Started - conversation_id: 01f0d4fb6c5519cebdab594d07e07c85, message_id: 01f0d4fb6c5f15b78960ee97549a1b31\n",
      "[Genie] Polling for result (max 120s)...\n",
      "[Genie] Poll #1 (0.1s): Status = FETCHING_METADATA\n",
      "[Genie] Poll #2 (2.2s): Status = ASKING_AI\n",
      "[Genie] Poll #3 (4.7s): Status = ASKING_AI\n",
      "[Genie] Poll #4 (7.7s): Status = COMPLETED\n",
      "[Genie] ‚úÖ Query completed!\n",
      "[Genie] Received response with 2 attachments\n",
      "[Genie] Attachment ID: 01f0d4fb6dea188d9914346a520aa67e\n",
      "[Genie] Calling query-result endpoint...\n",
      "[Genie] No statement_response in query result response\n",
      "[Genie] Final data rows extracted: 0\n",
      "[Genie] === Genie Query Complete ===\n",
      "\n",
      "üß† AGENT REASONING TRAIL:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ USER:\n",
      "   Analyze this support ticket and provide recommendations: Need to integrate our system with Salesforce API for syncing customer data...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   I'll analyze this support ticket step by step to provide you with the best recommendations....\n",
      "   üîß Calling tool: classify_ticket\n",
      "   üì• Input: {'ticket_text': 'Need to integrate our system with Salesforce API for syncing customer data'}...\n",
      "\n",
      "üì§ TOOL RESULT (classify_ticket):\n",
      "   {\n",
      "  \"category\": \"Software\",\n",
      "  \"priority\": \"P3\",\n",
      "  \"assigned_team\": \"Applications\",\n",
      "  \"confidence\": \"0.0\"\n",
      "}...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   Now let me search the knowledge base for relevant integration documentation and guides:...\n",
      "   üîß Calling tool: search_knowledge\n",
      "   üì• Input: {'query': 'Salesforce API integration customer data sync'}...\n",
      "\n",
      "üì§ TOOL RESULT (search_knowledge):\n",
      "   [\n",
      "  {\n",
      "    \"doc_id\": \"Guide_STORAGE_BACKUP_GUIDE_004\",\n",
      "    \"doc_type\": \"Guide\",\n",
      "    \"title\": \"STORAGE AND BACKUP PROCEDURES - CLOUD STORAGE SYNC (Part 4)\",\n",
      "    \"content\": \"------------------\\n\\nFiles N...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   The knowledge base results aren't very specific to Salesforce API integration. Let me search for more targeted API integration documentation:...\n",
      "   üîß Calling tool: search_knowledge\n",
      "   üì• Input: {'query': 'API integration third-party system data synchronization'}...\n",
      "\n",
      "üì§ TOOL RESULT (search_knowledge):\n",
      "   [\n",
      "  {\n",
      "    \"doc_id\": \"Guide_APPLICATION_SUPPORT_GUIDE_004\",\n",
      "    \"doc_type\": \"Guide\",\n",
      "    \"title\": \"APPLICATION SUPPORT GUIDE - SECTION 4: INTEGRATION ISSUES (Part 4)\",\n",
      "    \"content\": \"-----------------...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   Great! I found relevant integration documentation. Let me also check if there are any historical tickets about Salesforce integrations to see what solutions have worked before:...\n",
      "   üîß Calling tool: query_historical\n",
      "   üì• Input: {'question': 'What are common solutions and approaches for Salesforce API integration projects for customer data synchronization?'}...\n",
      "\n",
      "üì§ TOOL RESULT (query_historical):\n",
      "   {\n",
      "  \"text\": \"What are common solutions and approaches for Salesforce API integration projects for customer data synchronization?\",\n",
      "  \"query\": null,\n",
      "  \"data\": null,\n",
      "  \"conversation_id\": \"01f0d4fb6c5519...\n",
      "\n",
      "ü§ñ AGENT FINAL ANSWER:\n",
      "   ## Analysis and Recommendations\n",
      "\n",
      "Based on my analysis of your support ticket, here are my findings and recommendations:\n",
      "\n",
      "### **Ticket Classification**\n",
      "- **Category**: Software\n",
      "- **Priority**: P3 (Medium priority)\n",
      "- **Assigned Team**: Applications Team\n",
      "- **Type**: Integration Request\n",
      "\n",
      "### **Key Recommendations**\n",
      "\n",
      "#### 1. **Follow Integration Best Practices**\n",
      "Based on the knowledge base documentation for third-party integrations:\n",
      "- This falls under \"Third-Party Integration\" which is typically P2 p...\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   ‚è±Ô∏è  Time: 26.38s\n",
      "   üß∞ Tools used: 3/4 - query_historical, search_knowledge, classify_ticket\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[{\"trace_id\": \"tr-aad9bfbf1fb6158e477c473ac147ffc9\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-dc1f637d45618513aeebf427619bd7a1\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-9ee27d70c27884d9fcebf02dca6dd06c\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-4577681fa7ae468ad65fbfd4983cc635\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-839085ce50579d8c405af171768f9d16\", \"sql_warehouse_id\": null}]",
      "text/plain": [
       "[Trace(trace_id=tr-aad9bfbf1fb6158e477c473ac147ffc9), Trace(trace_id=tr-dc1f637d45618513aeebf427619bd7a1), Trace(trace_id=tr-9ee27d70c27884d9fcebf02dca6dd06c), Trace(trace_id=tr-4577681fa7ae468ad65fbfd4983cc635), Trace(trace_id=tr-839085ce50579d8c405af171768f9d16)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"üß™ TEST 3: FEATURE REQUEST\")\n",
    "print(\"Expected behavior: Should use classify + extract, maybe search\\n\")\n",
    "\n",
    "result_3 = run_agent_test(\"Need to integrate our system with Salesforce API for syncing customer data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b5a0689-a18e-4aee-9901-ef9d721c2370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 5: Sequential vs Agent Comparison\n",
    "\n",
    "Compare the agent approach with the current sequential pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ebf47a0-1da0-470f-b425-137256ee5c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÑ Sequential Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6664dc71-6b2c-4236-b0b3-45fcb619b84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sequential_pipeline(claim_text: str):\n",
    "    \"\"\"\n",
    "    Run all 4 tools sequentially (current dashboard approach).\n",
    "    Always executes all tools regardless of claim complexity.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üîÑ SEQUENTIAL PIPELINE - Running ALL tools\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = {}\n",
    "    \n",
    "    # Step 1: Classify\n",
    "    print(\"1Ô∏è‚É£ Step 1: Classifying claim...\")\n",
    "    results['classification'] = call_uc_function(\"fraud_classify\", {\"claim_text\": claim_text})\n",
    "    print(f\"   ‚úÖ Done\\n\")\n",
    "    \n",
    "    # Step 2: Extract\n",
    "    print(\"2Ô∏è‚É£ Step 2: Extracting metadata...\")\n",
    "    results['metadata'] = call_uc_function(\"fraud_extract_indicators\", {\"claim_text\": claim_text})\n",
    "    print(f\"   ‚úÖ Done\\n\")\n",
    "    \n",
    "    # Step 3: Search\n",
    "    print(\"3Ô∏è‚É£ Step 3: Searching fraud knowledge base...\")\n",
    "    results['knowledge'] = search_knowledge_base(claim_text)\n",
    "    print(f\"   ‚úÖ Done\\n\")\n",
    "    \n",
    "    # Step 4: Genie\n",
    "    print(\"4Ô∏è‚É£ Step 4: Querying historical claims via Genie...\")\n",
    "    genie = GenieConversationTool(w, GENIE_SPACE_ID)\n",
    "    results['historical'] = genie.query(f\"Find similar resolved claims about: {claim_text}\")\n",
    "    print(f\"   ‚úÖ Done\\n\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä SEQUENTIAL SUMMARY:\")\n",
    "    print(f\"   ‚è±Ô∏è  Time: {elapsed_time:.2f}s\")\n",
    "    print(f\"   üß∞ Tools used: 4/4 (always all tools)\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    return results, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f006197b-134c-4661-9127-bf7941ab8ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üÜö Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15eb0ee4-a491-43a8-b2d2-54e08c2773a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜö\n",
      "COMPARISON TEST: Sequential vs Agent\n",
      "üÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜö\n",
      "\n",
      "üìã Testing with ticket:\n",
      "   'How do I export a report to PDF?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üÖ∞Ô∏è APPROACH A: SEQUENTIAL PIPELINE (Current Dashboard)\n",
      "\n",
      "================================================================================\n",
      "üîÑ SEQUENTIAL PIPELINE - Running ALL tools\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Step 1: Classifying ticket...\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_classify\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_classify('How do I export a report to PDF?') as result...\n",
      "‚úÖ UC Function result received\n",
      "   ‚úÖ Done\n",
      "\n",
      "2Ô∏è‚É£ Step 2: Extracting metadata...\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_extract\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_extract('How do I export a report to PDF?') as result...\n",
      "‚úÖ UC Function result received\n",
      "   ‚úÖ Done\n",
      "\n",
      "3Ô∏è‚É£ Step 3: Searching knowledge base...\n",
      "üîç Searching knowledge base: 'How do I export a report to PDF?...'\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "‚úÖ Found 3 documents\n",
      "   ‚úÖ Done\n",
      "\n",
      "4Ô∏è‚É£ Step 4: Querying historical tickets via Genie...\n",
      "[Genie] Initialized with space_id: 01f0d3b893281964829ca73bcfa4d7c7\n",
      "\n",
      "[Genie] === Starting Genie Query ===\n",
      "[Genie] Question: Find similar resolved tickets about: How do I export a report to PDF?...\n",
      "[Genie] Starting conversation...\n",
      "[Genie] ‚úÖ Started - conversation_id: 01f0d4fb7676147a8fbefb88ab3bb1e6, message_id: 01f0d4fb76811bbfaa1d2117487245cd\n",
      "[Genie] Polling for result (max 120s)...\n",
      "[Genie] Poll #1 (0.1s): Status = FETCHING_METADATA\n",
      "[Genie] Poll #2 (2.2s): Status = ASKING_AI\n",
      "[Genie] Poll #3 (4.7s): Status = PENDING_WAREHOUSE\n",
      "[Genie] Poll #4 (7.7s): Status = PENDING_WAREHOUSE\n",
      "[Genie] Poll #5 (11.3s): Status = COMPLETED\n",
      "[Genie] ‚úÖ Query completed!\n",
      "[Genie] Received response with 2 attachments\n",
      "[Genie] Attachment ID: 01f0d4fb786e1148a455fbbb8f7f0df5\n",
      "[Genie] Extracted SQL: SELECT `ticket_id`, `subject`, `description`, `submitted_date`\n",
      "FROM `langraph_demo`.`agents`.`sample...\n",
      "[Genie] Calling query-result endpoint...\n",
      "[Genie] Found statement_response\n",
      "[Genie] Found columns: ['ticket_id', 'subject', 'description', 'submitted_date']\n",
      "[Genie] Found 1 data rows from Genie API\n",
      "[Genie] ‚úÖ Successfully converted 1 rows\n",
      "[Genie] Final data rows extracted: 1\n",
      "[Genie] Method: Genie query-result API\n",
      "[Genie] === Genie Query Complete ===\n",
      "\n",
      "   ‚úÖ Done\n",
      "\n",
      "================================================================================\n",
      "üìä SEQUENTIAL SUMMARY:\n",
      "   ‚è±Ô∏è  Time: 13.32s\n",
      "   üß∞ Tools used: 4/4 (always all tools)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üÖ±Ô∏è APPROACH B: LANGGRAPH AGENT (New Adaptive)\n",
      "\n",
      "================================================================================\n",
      "üé´ TICKET: How do I export a report to PDF?\n",
      "================================================================================\n",
      "\n",
      "üîß Calling UC Function: langraph_demo.agents.ai_classify\n",
      "  üìù SQL: SELECT langraph_demo.agents.ai_classify('How do I export a report to PDF?') as result...\n",
      "‚úÖ UC Function result received\n",
      "üîç Searching knowledge base: 'export report to PDF how to guide steps...'\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "‚úÖ Found 3 documents\n",
      "üîç Searching knowledge base: 'PDF export report download print save file format...'\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "‚úÖ Found 3 documents\n",
      "üß† AGENT REASONING TRAIL:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ USER:\n",
      "   Analyze this support ticket and provide recommendations: How do I export a report to PDF?...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   I'll analyze this support ticket step by step. Let me start by classifying it to understand its nature, then determine what additional information would be most helpful....\n",
      "   üîß Calling tool: classify_ticket\n",
      "   üì• Input: {'ticket_text': 'How do I export a report to PDF?'}...\n",
      "\n",
      "üì§ TOOL RESULT (classify_ticket):\n",
      "   {\n",
      "  \"category\": \"Software\",\n",
      "  \"priority\": \"P3\",\n",
      "  \"assigned_team\": \"Applications\",\n",
      "  \"confidence\": \"0.0\"\n",
      "}...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   Based on the classification, this is a low-priority (P3) software question about application functionality. This appears to be a straightforward how-to question, so the knowledge base should have the most relevant documentation and step-by-step guides....\n",
      "   üîß Calling tool: search_knowledge\n",
      "   üì• Input: {'query': 'export report to PDF how to guide steps'}...\n",
      "\n",
      "üì§ TOOL RESULT (search_knowledge):\n",
      "   [\n",
      "  {\n",
      "    \"doc_id\": \"Playbook_SECURITY_INCIDENT_PLAYBOOK_006\",\n",
      "    \"doc_type\": \"Playbook\",\n",
      "    \"title\": \"SECURITY INCIDENT RESPONSE PLAYBOOK - PHASE 5: LESSONS LEARNED (Part 6)\",\n",
      "    \"content\": \"Objec...\n",
      "\n",
      "ü§ñ AGENT THOUGHT & ACTION:\n",
      "   The knowledge base search didn't return specific PDF export instructions. Let me try a more targeted search for PDF export functionality....\n",
      "   üîß Calling tool: search_knowledge\n",
      "   üì• Input: {'query': 'PDF export report download print save file format'}...\n",
      "\n",
      "üì§ TOOL RESULT (search_knowledge):\n",
      "   [\n",
      "  {\n",
      "    \"doc_id\": \"Guide_APPLICATION_SUPPORT_GUIDE_003\",\n",
      "    \"doc_type\": \"Guide\",\n",
      "    \"title\": \"APPLICATION SUPPORT GUIDE - SECTION 3: DATA & REPORTING ISSUES (Part 3)\",\n",
      "    \"content\": \"------------...\n",
      "\n",
      "ü§ñ AGENT FINAL ANSWER:\n",
      "   ## Analysis and Recommendations\n",
      "\n",
      "**Ticket Classification:**\n",
      "- **Category:** Software\n",
      "- **Priority:** P3 (Low - Standard how-to question)\n",
      "- **Assigned Team:** Applications\n",
      "- **Type:** How-to/Documentation request\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1. **Immediate Action:** This is a straightforward how-to question that should be resolved quickly with step-by-step instructions.\n",
      "\n",
      "2. **Knowledge Gap Identified:** The knowledge base search didn't return specific PDF export documentation, indicating a potential ga...\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   ‚è±Ô∏è  Time: 17.92s\n",
      "   üß∞ Tools used: 2/4 - search_knowledge, classify_ticket\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜö\n",
      "COMPARISON RESULTS\n",
      "üÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜöüÜö\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Metric                  ‚îÇ Sequential       ‚îÇ Agent            ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Tools Used              ‚îÇ 4/4 (100%)       ‚îÇ 2/4 (50%)        ‚îÇ\n",
      "‚îÇ Efficiency              ‚îÇ Fixed pipeline   ‚îÇ Adaptive         ‚îÇ\n",
      "‚îÇ Predictability          ‚îÇ High             ‚îÇ Medium           ‚îÇ\n",
      "‚îÇ Flexibility             ‚îÇ None             ‚îÇ High             ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üí° INSIGHTS:\n",
      "   ‚úÖ Agent was more efficient: used 2/4 tools vs 4/4\n",
      "   ‚úÖ Agent skipped: {'query_historical', 'extract_metadata'}\n",
      "   üí∞ Potential cost savings: ~50%\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[{\"trace_id\": \"tr-10f96eb93944543908ee1c9c7beb64cf\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-89dfa8663a11e0e683a0cbe60c706077\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-e36c30f7fd79bafa5f2369b3b32e823b\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-fc74dd85e7615be0f743340a39499085\", \"sql_warehouse_id\": null}]",
      "text/plain": [
       "[Trace(trace_id=tr-10f96eb93944543908ee1c9c7beb64cf), Trace(trace_id=tr-89dfa8663a11e0e683a0cbe60c706077), Trace(trace_id=tr-e36c30f7fd79bafa5f2369b3b32e823b), Trace(trace_id=tr-fc74dd85e7615be0f743340a39499085)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"üÜö\" * 40)\n",
    "print(\"COMPARISON TEST: Sequential vs Agent\")\n",
    "print(\"üÜö\" * 40 + \"\\n\")\n",
    "\n",
    "comparison_claim = \"How do I export a report to PDF?\"\n",
    "\n",
    "print(\"üìã Testing with claim:\")\n",
    "print(f\"   '{comparison_claim}'\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Run Sequential\n",
    "print(\"\\nüÖ∞Ô∏è APPROACH A: SEQUENTIAL PIPELINE (Current Dashboard)\")\n",
    "sequential_result, seq_time = sequential_pipeline(comparison_claim)\n",
    "\n",
    "# Run Agent\n",
    "print(\"\\nüÖ±Ô∏è APPROACH B: LANGGRAPH AGENT (New Adaptive)\")\n",
    "agent_result = run_agent_test(comparison_claim, show_reasoning=True)\n",
    "\n",
    "# Extract agent time and tools\n",
    "agent_tools = [getattr(m, 'name', None) for m in agent_result['messages'] if getattr(m, 'type', None) == 'tool']\n",
    "agent_tools_unique = list(set([t for t in agent_tools if t]))\n",
    "\n",
    "print(\"\\n\" + \"üÜö\" * 40)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"üÜö\" * 40 + \"\\n\")\n",
    "\n",
    "print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"‚îÇ Metric                  ‚îÇ Sequential       ‚îÇ Agent            ‚îÇ\")\n",
    "print(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "print(f\"‚îÇ Tools Used              ‚îÇ 4/4 (100%)       ‚îÇ {len(agent_tools_unique)}/4 ({len(agent_tools_unique)*25}%)        ‚îÇ\")\n",
    "print(f\"‚îÇ Efficiency              ‚îÇ Fixed pipeline   ‚îÇ Adaptive         ‚îÇ\")\n",
    "print(f\"‚îÇ Predictability          ‚îÇ High             ‚îÇ Medium           ‚îÇ\")\n",
    "print(f\"‚îÇ Flexibility             ‚îÇ None             ‚îÇ High             ‚îÇ\")\n",
    "print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "\n",
    "print(\"\\nüí° INSIGHTS:\")\n",
    "if len(agent_tools_unique) < 4:\n",
    "    print(f\"   ‚úÖ Agent was more efficient: used {len(agent_tools_unique)}/4 tools vs 4/4\")\n",
    "    print(f\"   ‚úÖ Agent skipped: {set(['classify_claim', 'extract_metadata', 'search_knowledge', 'query_historical']) - set(agent_tools_unique)}\")\n",
    "    print(f\"   üí∞ Potential cost savings: ~{(4-len(agent_tools_unique))*25}%\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è  Agent used all tools for this complex claim\")\n",
    "    print(f\"   ‚ÑπÔ∏è  For simple claims, agent would be more efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8750d1d-b1e8-44d5-b800-3fc6fbe5003d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# ‚úÖ Summary & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e00b45-4c4d-4c60-8e12-f2a8cc33ff45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéâ Success Criteria Checklist\n",
    "\n",
    "If you've successfully run all cells above, you have:\n",
    "\n",
    "- ‚úÖ **Tool 1 Working:** UC Function (fraud_classify) with WorkspaceClient\n",
    "- ‚úÖ **Tool 2 Working:** UC Function (fraud_extract_indicators) with WorkspaceClient\n",
    "- ‚úÖ **Tool 3 Working:** Vector Search with WorkspaceClient\n",
    "- ‚úÖ **Tool 4 Working:** Genie Conversation API with WorkspaceClient\n",
    "- ‚úÖ **All 4 LangChain Tools Created:** Successfully wrapped APIs\n",
    "- ‚úÖ **LangGraph ReAct Agent Created:** Agent with 4 tools and system prompt\n",
    "- ‚úÖ **Agent Makes Decisions:** Tested with different claim complexities\n",
    "- ‚úÖ **Comparison Complete:** Sequential vs Agent side-by-side\n",
    "- ‚úÖ **Code is Portable:** Everything uses WorkspaceClient (same as dashboard)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Key Learnings\n",
    "\n",
    "**What We Learned:**\n",
    "1. **Agent adapts to claim complexity** - uses fewer tools for simple questions\n",
    "2. **Tool descriptions matter** - they guide the agent's decision-making\n",
    "3. **System prompt is critical** - sets the agent's behavior and efficiency\n",
    "4. **ReAct pattern works** - Thought ‚Üí Action ‚Üí Observation loop is visible\n",
    "5. **Trade-offs are real** - flexibility vs predictability\n",
    "\n",
    "**When to Use Each Approach:**\n",
    "- **Sequential:** Predictable, uniform claims; need consistency\n",
    "- **Agent:** Varied complexity claims; want cost optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Phase 2: Dashboard Integration**\n",
    "\n",
    "Now that the agent works in the notebook, we can:\n",
    "\n",
    "1. ‚úÖ Extract working code to `dashboard/langraph_agent.py` module\n",
    "2. ‚úÖ Add 5th tab to dashboard: \"üß™ LangGraph Agent (Experimental)\"\n",
    "3. ‚úÖ Display agent reasoning trail in Streamlit UI\n",
    "4. ‚úÖ Add comparison mode in dashboard\n",
    "5. ‚úÖ Deploy and test in production\n",
    "\n",
    "**Ready to move to dashboard integration!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f117381e-4689-48ab-bc31-cd6a3cd82324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                                                                ‚ïë\n",
      "‚ïë  üéâ LANGGRAPH AGENT NOTEBOOK COMPLETE! üéâ                     ‚ïë\n",
      "‚ïë                                                                ‚ïë\n",
      "‚ïë  ‚úÖ All 4 tools tested and working                            ‚ïë\n",
      "‚ïë  ‚úÖ LangGraph ReAct Agent created                             ‚ïë\n",
      "‚ïë  ‚úÖ Agent makes intelligent decisions                         ‚ïë\n",
      "‚ïë  ‚úÖ Sequential vs Agent comparison done                       ‚ïë\n",
      "‚ïë  ‚úÖ Code is ready for dashboard integration                   ‚ïë\n",
      "‚ïë                                                                ‚ïë\n",
      "‚ïë  üìç Branch: agent_langraph_trying                             ‚ïë\n",
      "‚ïë  üìÅ Notebook: notebooks/23_langraph_agent_learning.py         ‚ïë\n",
      "‚ïë                                                                ‚ïë\n",
      "‚ïë  üöÄ Ready for Phase 2: Dashboard Integration!                 ‚ïë\n",
      "‚ïë                                                                ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                                                                ‚ïë\n",
    "‚ïë  üéâ LANGGRAPH AGENT NOTEBOOK COMPLETE! üéâ                     ‚ïë\n",
    "‚ïë                                                                ‚ïë\n",
    "‚ïë  ‚úÖ All 4 tools tested and working                            ‚ïë\n",
    "‚ïë  ‚úÖ LangGraph ReAct Agent created                             ‚ïë\n",
    "‚ïë  ‚úÖ Agent makes intelligent decisions                         ‚ïë\n",
    "‚ïë  ‚úÖ Sequential vs Agent comparison done                       ‚ïë\n",
    "‚ïë  ‚úÖ Code is ready for dashboard integration                   ‚ïë\n",
    "‚ïë                                                                ‚ïë\n",
    "‚ïë  üìç Branch: agent_langraph_trying                             ‚ïë\n",
    "‚ïë  üìÅ Notebook: notebooks/23_langraph_agent_learning.py         ‚ïë\n",
    "‚ïë                                                                ‚ïë\n",
    "‚ïë  üöÄ Ready for Phase 2: Dashboard Integration!                 ‚ïë\n",
    "‚ïë                                                                ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_langraph_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
